{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4fd6b7550484ed709bca7da18028042",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Wednesday at 15:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to two people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bcfc9fec57b3191b04b5977dfff4f75",
     "grade": false,
     "grade_id": "SpacyIntro_ASpacyIntro_BSpacyIntro_CSpacyIntro_DSpacyIntro_E_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Introduction to spaCy\n",
    "\n",
    "SpaCy is a tool that does tokenization, parsing, tagging and named entity regocnition (among other things).\n",
    "\n",
    "When we parse a document via spaCy, we get an object that holds sentences and tokens, as well as their POS tags, dependency relations and so on.\n",
    "\n",
    "Look at the next cell for an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32e3514f2f346e23815caffc075f05ed",
     "grade": false,
     "grade_id": "SpacyIntro_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy PROPN nsubj False\n",
      "is AUX ROOT True\n",
      "capable ADJ acomp False\n",
      "of ADP prep True\n",
      "    SPACE  False\n",
      "tagging NOUN pobj False\n",
      ", PUNCT punct False\n",
      "parsing VERB conj False\n",
      "and CCONJ cc True\n",
      "annotating VERB conj False\n",
      "text NOUN dobj False\n",
      ". PUNCT punct False\n",
      "It PRON nsubj True\n",
      "recognizes VERB ROOT False\n",
      "sentences NOUN dobj False\n",
      "and CCONJ cc True\n",
      "stop VERB conj False\n",
      "words NOUN dobj False\n",
      ". PUNCT punct False\n",
      "------------------------------\n",
      "The nouns and proper nouns in this text are:\n",
      "SpaCy\n",
      "tagging\n",
      "text\n",
      "sentences\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "# Our sample input\n",
    "text = 'SpaCy is capable of    tagging, parsing and annotating text. It recognizes sentences and stop words.'\n",
    "\n",
    "# Parse the sample input\n",
    "doc = nlp(text)\n",
    "\n",
    "# For every sentence\n",
    "for sent in doc.sents:\n",
    "    # For every token\n",
    "    for token in sent:\n",
    "        # Print the token itself, the pos tag, \n",
    "        # dependency tag and whether spacy thinks this is a stop word\n",
    "        print(token, token.pos_, token.dep_, token.is_stop)\n",
    "        \n",
    "print('-'*30)\n",
    "print('The nouns and proper nouns in this text are:')\n",
    "# Print only the nouns:\n",
    "for token in doc:\n",
    "    if token.pos_ in ['NOUN', 'PROPN']:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2eaf9bed1ec41ba3dafd392c8aae894",
     "grade": false,
     "grade_id": "SpacyIntro_A_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## SpaCy A) [5 points]\n",
    "### Splitting text into sentences\n",
    "\n",
    "You are given the text in the next cell.\n",
    "\n",
    "```\n",
    "text = '''\n",
    "This is a sentence. \n",
    "Mr. A. said this was another! \n",
    "But is this a sentence? \n",
    "The abbreviation Merch. means merchant(s).\n",
    "At certain univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "```\n",
    "\n",
    "Use spaCy to split this into sentences. Store the resulting sentences (each as a **single** string) in the list ```sentences```. Make sure to convert the tokens to strings (e.g. via str(token))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90cbf65cd82368f51da27e28808346b2",
     "grade": false,
     "grade_id": "SpacyIntro_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " This is a sentence .\n",
      ".\n",
      " Mr. A. said this was another ! \n",
      "\n",
      ".\n",
      " But is this a sentence ?\n",
      ".\n",
      " The abbreviation Merch . means merchant(s ) . \n",
      "\n",
      ".\n",
      " At certain Univ .\n",
      ".\n",
      " in the U.S. and U.K. they study NLP . \n",
      "\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English \n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "sentences = []\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    sentence = \"\"\n",
    "    for token in sent:\n",
    "        sentence += ' ' +str(token)\n",
    "    sentences.append(str(sentence))\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print('.')\n",
    "    assert type(sentence) == str, 'You need to convert this to a single string!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbbce90ee40e45a30a046006e4087da1",
     "grade": true,
     "grade_id": "test_SpacyIntro_A0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a49ebe93c151344970ab83081b2a413f",
     "grade": false,
     "grade_id": "SpacyIntro_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## SpaCy B) [5 points]\n",
    "\n",
    "### Cluster the text by POS tag\n",
    "\n",
    "Next we want to cluster the text by the corresponding part-of-speech (POS) tags. \n",
    "\n",
    "The result should be a dictionary ```pos_tags``` where the keys are the POS tags and the values are lists of words with those POS tags. Make sure your words are converted to **strings**.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "pos_tags['VERB'] # Output: ['said', 'means', 'study']\n",
    "pos_tags['ADJ']  # Output: ['certain']\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92630877dfeae43e9869b293e17da2b0",
     "grade": false,
     "grade_id": "SpacyIntro_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words with the POS tag SPACE are ['\\n', '\\n', '\\n', '\\n'].\n",
      "The words with the POS tag DET are ['This', 'a', 'this', 'another', 'this', 'a', 'The', 'the'].\n",
      "The words with the POS tag AUX are ['is', 'was', 'is'].\n",
      "The words with the POS tag NOUN are ['sentence', 'sentence', 'abbreviation'].\n",
      "The words with the POS tag PUNCT are ['.', '!', '?', '.', ')', '.', '.', '.'].\n",
      "The words with the POS tag PROPN are ['Mr.', 'A.', 'Merch', 'merchant(s', 'Univ', 'U.S.', 'U.K.', 'NLP'].\n",
      "The words with the POS tag VERB are ['said', 'means', 'study'].\n",
      "The words with the POS tag CCONJ are ['But', 'and'].\n",
      "The words with the POS tag ADP are ['At', 'in'].\n",
      "The words with the POS tag ADJ are ['certain'].\n",
      "The words with the POS tag PRON are ['they'].\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "pos_tags = dict()\n",
    "doc = nlp(text)\n",
    "# tag_ lists the fine-grained part of speech.\n",
    "# pos_ lists the coarse-grained part of speech.\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        pos = token.pos_\n",
    "        if pos in pos_tags:\n",
    "            pos_tags[pos].append(str(token))\n",
    "        else:\n",
    "            pos_tags[pos] = [str(token)]\n",
    "\n",
    "for key in pos_tags:\n",
    "    print('The words with the POS tag {} are {}.'.format(key, pos_tags[key]))\n",
    "    for token in pos_tags[key]:\n",
    "        assert type(token) == str, 'Each token should be a string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "401366109e209a58b9d6880f9d0efbd2",
     "grade": true,
     "grade_id": "test_SpacyIntro_B0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "764be63962b4a589a22a7118b6196948",
     "grade": false,
     "grade_id": "SpacyIntro_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SpaCy C) [5 points]\n",
    "\n",
    "### Stop word removal\n",
    "\n",
    "Stop words are words that appear often in a language and don't hold much meaning for a NLP task. Examples are the words ```a, to, the, this, has, ...```. This depends on the task and domain you are working on.\n",
    "\n",
    "SpaCy has its own internal list of stop words. Use spaCy to remove all stop words from the given text. Store your result as a **single string** in the variable ```stopwords_removed```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8ade4666109405cdaf97edbe7b9fb3f",
     "grade": false,
     "grade_id": "SpacyIntro_C",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " sentence . Mr. A. said ! \n",
      " sentence ? abbreviation Merch . means merchant(s ) . \n",
      " certain Univ . U.S. U.K. study NLP . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "stopwords_removed = ''\n",
    "\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        if not token.is_stop:\n",
    "            stopwords_removed += ' ' +str((token))\n",
    "\n",
    "print(stopwords_removed)\n",
    "assert type(stopwords_removed) == str, 'Your answer should be a single string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ede061b3d5f7730b3d8a61b49d24c668",
     "grade": true,
     "grade_id": "test_SpacyIntro_C0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09f8b638ddc3e45a920a8632957fb8da",
     "grade": false,
     "grade_id": "SpacyIntro_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SpaCy D) [2 points]\n",
    "\n",
    "### Dependency Tree\n",
    "\n",
    "We now want to use spaCy to visualize the dependency tree of a certain sentence. Look at the Jupyter Example on the [spaCy website](https://spacy.io/usage/visualizers/). Render the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "789a34dc995bc1fbcc1077b5a07edcbd",
     "grade": true,
     "grade_id": "SpacyIntro_D",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4103bbc426b94e968a6f0783c9ec9b42-0\" class=\"displacy\" width=\"890\" height=\"257.0\" direction=\"ltr\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Dependency</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">Parsing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">helpful</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">many</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">tasks.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4103bbc426b94e968a6f0783c9ec9b42-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,62.0 165.0,62.0 165.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4103bbc426b94e968a6f0783c9ec9b42-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4103bbc426b94e968a6f0783c9ec9b42-0-1\" stroke-width=\"2px\" d=\"M190,122.0 C190,62.0 285.0,62.0 285.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4103bbc426b94e968a6f0783c9ec9b42-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,124.0 L182,112.0 198,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4103bbc426b94e968a6f0783c9ec9b42-0-2\" stroke-width=\"2px\" d=\"M310,122.0 C310,62.0 405.0,62.0 405.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4103bbc426b94e968a6f0783c9ec9b42-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M405.0,124.0 L413.0,112.0 397.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4103bbc426b94e968a6f0783c9ec9b42-0-3\" stroke-width=\"2px\" d=\"M430,122.0 C430,62.0 525.0,62.0 525.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4103bbc426b94e968a6f0783c9ec9b42-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M525.0,124.0 L533.0,112.0 517.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4103bbc426b94e968a6f0783c9ec9b42-0-4\" stroke-width=\"2px\" d=\"M670,122.0 C670,62.0 765.0,62.0 765.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4103bbc426b94e968a6f0783c9ec9b42-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,124.0 L662,112.0 678,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4103bbc426b94e968a6f0783c9ec9b42-0-5\" stroke-width=\"2px\" d=\"M550,122.0 C550,2.0 770.0,2.0 770.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4103bbc426b94e968a6f0783c9ec9b42-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770.0,124.0 L778.0,112.0 762.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = 'Dependency Parsing is helpful for many tasks.'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"dep\", options={\"distance\":120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a5b58876deddef3e637584ebfbace3c",
     "grade": false,
     "grade_id": "SpacyIntro_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SpaCy E) [5 points]\n",
    "\n",
    "### Dependency Parsing\n",
    "\n",
    "Use spaCy to extract all subjects and objects from the text. We define a subject as any word that has ```subj``` in its dependency tag (e.g. ```nsubj```, ```nsubjpass```, ...). Similarly we define an object as any token that has ```obj``` in its dependency tag (e.g. ```dobj```, ```pobj```, etc.).\n",
    "\n",
    "For each sentence extract the subject, root node ```ROOT``` of the tree and object and store them as a single string in a list. Name this list ```subj_obj```.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "text = 'Learning multiple ways of representing text is cool. We can access parts of the sentence with dependency tags.'\n",
    "\n",
    "subj_obj = ['Learning ways text is', 'We access parts sentence tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aa38180840293c1d230a57eb90d9ee7",
     "grade": false,
     "grade_id": "SpacyIntro_E",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  This is\n",
      "  A. said this\n",
      "  is this\n",
      "  abbreviation means\n",
      "  At Univ\n",
      "  U.S. they study NLP\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('/srv/shares/NLP/en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "subj_obj = []\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sentence = ' '\n",
    "    for token in sent:\n",
    "        if (\"subj\" in token.dep_) or (\"obj\" in token.dep_) or token.dep_ == \"ROOT\":\n",
    "            sentence += ' '+str(token)\n",
    "    subj_obj.append(sentence)\n",
    "           \n",
    "for cleaned_sent in subj_obj:\n",
    "    print(cleaned_sent)\n",
    "    assert type(cleaned_sent) == str, 'Each cleaned sentence should be a string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd16138bad9d0aced1ce7819d592c375",
     "grade": true,
     "grade_id": "test_SpacyIntro_E0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c4282d5e277c9573f5323515a68457d",
     "grade": false,
     "grade_id": "POS_Keywords_APOS_Keywords_B_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "In this assignment we want to write a keyword extractor. There are several methods of which we want to explore a few.\n",
    "\n",
    "We want to extract keywords from our Yelp reviews.\n",
    "\n",
    "##  POS tag based extraction\n",
    "\n",
    "When we look at keywords we realize that they are often combinations of nouns and adjectives. The idea is to find all sequences of nouns and adjectives in a corpus and count them. The $n$ most frequent ones are then our keywords.\n",
    "\n",
    "A keyword (or keyphrase) by this definition is any combination of nouns (NOUN) and adjectives (ADJ) that ends in a noun. We also count proper nouns (PROPN) as nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33f153a362b27cca8a0c0b6d8e54bde4",
     "grade": false,
     "grade_id": "POS_Keywords_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## POS tag based extraction A) [35 points]\n",
    "\n",
    "### POSKeywordExtractor\n",
    "\n",
    "Please complete the function ```keywords``` in the class ```POSKeywordExtractor```.\n",
    "\n",
    "You are given the file ```wiki_nlp.txt```, which has the raw text from all top-level Wikipedia pages under the category ```Natural language processing```. Use this for extracting your keywords.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "Let us look at the definition of an index term or keyword from Wikipedia. Here I highlighted all combinations of nouns and adjectives that end in a noun. All the highlighted words are potential keywords.\n",
    "\n",
    "An **index term**, **subject term**, **subject heading**, or **descriptor**, in **information retrieval**, is a **term** that captures the **essence** of the **topic** of a **document**. **Index terms** make up a **controlled vocabulary** for **use** in **bibliographic records**.\n",
    "\n",
    "*Rules:*\n",
    "\n",
    "- A keyphrase is a sequence of nouns, adjectives and proper nouns ending in a noun or proper noun.\n",
    "- Keywords / Keyphrases can not go over sentence boundaries.\n",
    "- We always take the longest sequence of nouns, adjectives and proper nouns\n",
    "  - Consider the sentence ```She studies natural language processing.```. The only extracted keyphrase here will be ```('natural', 'language', 'processing')```.\n",
    "- Consider the sentence ```neural networks massively increased the performance.```:\n",
    "  - Here our keyphrase would be ```neural networks```, not ```neural networks massively```.\n",
    "  - Our keyphrases are always the longest sequence of nouns and adjectives ending in a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4bd75393770930b4ff0803bdc433d48",
     "grade": false,
     "grade_id": "POS_Keywords_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('words',) appears 353 times.\n",
      "The keyword ('text',) appears 344 times.\n",
      "The keyword ('example',) appears 263 times.\n",
      "The keyword ('word',) appears 233 times.\n",
      "The keyword ('natural', 'language', 'processing') appears 184 times.\n",
      "The keyword ('documents',) appears 162 times.\n",
      "The keyword ('language',) appears 148 times.\n",
      "The keyword ('information',) appears 137 times.\n",
      "The keyword ('n',) appears 136 times.\n",
      "The keyword ('set',) appears 133 times.\n",
      "The keyword ('system',) appears 120 times.\n",
      "The keyword ('t',) appears 118 times.\n",
      "The keyword ('sentence',) appears 114 times.\n",
      "The keyword ('number',) appears 112 times.\n",
      "The keyword ('context',) appears 112 times.\n",
      "CPU times: user 32.9 s, sys: 2.98 s, total: 35.8 s\n",
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from typing import List, Tuple, Iterable\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "import pickle\n",
    "\n",
    "\n",
    "class POSKeywordExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load(\n",
    "            '/srv/shares/NLP/en_core_web_sm', \n",
    "            disable=['ner', 'parser']\n",
    "        )\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "        \n",
    "    def validate_keyphrase(self, candidate: Iterable[Token]) -> Iterable[Token]:\n",
    "        '''\n",
    "        Takes in a list of tokens which are all proper nouns, nouns or adjectives\n",
    "        and returns the longest sequence that ends in a proper noun or noun\n",
    "        \n",
    "        Args:\n",
    "            candidate         -- List of spacy tokens\n",
    "        Returns:\n",
    "            longest_keyphrase -- The longest sequence that ends in a noun\n",
    "                                 or proper noun\n",
    "                                 \n",
    "        Example:\n",
    "            candidate = [neural, networks, massively]\n",
    "            longest_keyphrase = [neural, networks]\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text       -- the raw text from which to extract keywords\n",
    "            n_keywords -- the number of keywords to return\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "        '''\n",
    "        doc = self.nlp(text)\n",
    "        keywords = []\n",
    "        \n",
    "        # Get all possible keywords\n",
    "        possible_keywords = []\n",
    "        for sent in doc.sents:\n",
    "            index = -1\n",
    "            # Iterate over sentences\n",
    "            for num_sent,token in enumerate(sent):\n",
    "                if(len(token) >= min_words):\n",
    "                    # Checking if it is a noun or propernoun\n",
    "                    if token.pos_ in ['NOUN', 'PROPN']:\n",
    "                        if(index == -1): \n",
    "                            possible_keywords.append(str(token))\n",
    "                            index = num_sent\n",
    "                        else:\n",
    "                            if(sent[num_sent-1].pos_ != \"ADJ\"):\n",
    "                                possible_keywords = possible_keywords[:-1]\n",
    "                            possible_keywords.append(str(sent[index:num_sent+1])) \n",
    "                    else:\n",
    "                        # Checking if keyword is adjective\n",
    "                        if(token.pos_ ==  \"ADJ\"):\n",
    "                            if(index == -1):\n",
    "                                index = num_sent\n",
    "                        else:\n",
    "                            index = -1\n",
    "                else:\n",
    "                    index = -1\n",
    "        \n",
    "        # Iterate over the possible keywords having minimum length\n",
    "        # Making dictionary of those keywords for extracting the topn n most frequent keywords\n",
    "        possible_keywords_min_length = [poss_key for poss_key in possible_keywords if len(poss_key.split(\" \")) >= min_words]\n",
    "        dict_of_keywords = {num_sent : possible_keywords_min_length.count(num_sent) for num_sent in possible_keywords_min_length}\n",
    "        \n",
    "        # Extract the top n most frequent keywords from the text\n",
    "        high = Counter(dict_of_keywords).most_common(n_keywords) \n",
    "        for n in high: \n",
    "            keywords.append((tuple(n[0].split()),n[1]))\n",
    "        return keywords\n",
    "        \n",
    "    \n",
    "with open('/srv/shares/NLP/wiki_nlp.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "\n",
    "keywords = POSKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('words',) appears 353 times.\n",
    "The keyword ('text',) appears 342 times.\n",
    "The keyword ('example',) appears 263 times.\n",
    "The keyword ('word',) appears 231 times.\n",
    "The keyword ('natural', 'language', 'processing') appears 184 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b4561146a35861783b7ff33b6f88418",
     "grade": true,
     "grade_id": "test_POS_Keywords_A0",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d41818baad631b02348797225ecb10e",
     "grade": false,
     "grade_id": "POS_Keywords_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### POS tag based extraction B) [4 points]\n",
    "\n",
    "Rerun the keyword extrator with a minimum word count of ```min_words=2``` and a keyword count of ```n_keywords=15```.\n",
    "\n",
    "Store this in the variable ```keywords_2```. Print the result.\n",
    "\n",
    "Make sure to convert the input text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2c992b8ff2e84e9e54e9ec43532e0b3",
     "grade": false,
     "grade_id": "POS_Keywords_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('natural', 'language', 'processing'), 184),\n",
       " (('computational', 'linguistics'), 106),\n",
       " (('external', 'links'), 101),\n",
       " (('machine', 'translation'), 94),\n",
       " (('information', 'retrieval'), 71),\n",
       " (('natural', 'language'), 66),\n",
       " (('sentiment', 'analysis'), 58),\n",
       " (('text', 'mining'), 55),\n",
       " (('artificial', 'intelligence'), 50),\n",
       " (('word', 'sense', 'disambiguation'), 47),\n",
       " (('computer', 'science'), 36),\n",
       " (('machine', 'learning'), 35),\n",
       " (('information', 'extraction'), 33),\n",
       " (('speech', 'recognition'), 31),\n",
       " (('document', 'summarization'), 30)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_2 = []\n",
    "\n",
    "with open('/srv/shares/NLP/wiki_nlp.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "\n",
    "keywords_2 = POSKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=2)\n",
    "\n",
    "keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a766c838e3206e322155af855456f4b9",
     "grade": true,
     "grade_id": "test_POS_Keywords_B0",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dba1ccbd96865d0c64ca24b2dd8a3bff",
     "grade": false,
     "grade_id": "Stopword_Keywords_AStopword_Keywords_B_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Stop word based keyword extraction\n",
    "\n",
    "One approach to extract keywords is by splitting the text at the stop words. Then we count these potential keywords and output the top $n$ keywords. Make sure to only include words proper words. Here we define proper words as those words that match the regular expression ```r'\\b(\\W+|\\w+)\\b'```. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8169155e9883ba38ca52d261d3e4b771",
     "grade": false,
     "grade_id": "Stopword_Keywords_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Stop word based keyword extraction A) [35 points]\n",
    "\n",
    "Complete the function ```keywords``` in the class ```StopWordKeywordExtractor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5aac28b78aebbabdb0b077e292efc5c",
     "grade": false,
     "grade_id": "Stopword_Keywords_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword  words appears 273 times.\n",
      "The keyword  text appears 263 times.\n",
      "The keyword  example appears 257 times.\n",
      "The keyword  word appears 201 times.\n",
      "The keyword  references appears 184 times.\n",
      "The keyword  natural language processing appears 165 times.\n",
      "The keyword  n appears 160 times.\n",
      "The keyword  use appears 151 times.\n",
      "The keyword  set appears 144 times.\n",
      "The keyword  language appears 123 times.\n",
      "The keyword  t appears 120 times.\n",
      "The keyword  documents appears 118 times.\n",
      "The keyword  based appears 115 times.\n",
      "The keyword  1 appears 115 times.\n",
      "The keyword  number appears 106 times.\n",
      "CPU times: user 7.41 s, sys: 2.94 s, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "class StopWordKeywordExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load('/srv/shares/NLP/en_core_web_sm', disable=['ner', 'parser'])\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "        \n",
    "    def is_proper_word(self, token:str) -> bool:\n",
    "        '''\n",
    "        Checks if the word is a proper word by our definition\n",
    "        \n",
    "        Arguments:\n",
    "            token     -- The token as a string\n",
    "        Return:\n",
    "            is_proper -- True / False\n",
    "        '''\n",
    "        match = re.search(r'\\b(\\W+|\\w+)\\b', token)\n",
    "        return match and token == match[0] \n",
    "    \n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text       -- the raw text from which to extract keywords\n",
    "            n_keywords -- the number of keywords to return\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "                          Example: [(('potato'), 12), (('potato', 'harvesting'), 9), ...]\n",
    "        '''\n",
    "        doc = self.nlp(text)\n",
    "        keywords = []\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        keyword = ''\n",
    "        \n",
    "        for sent in doc.sents:  \n",
    "            for token in sent:\n",
    "                if(not token.is_stop and self.is_proper_word(str(token))):  \n",
    "                    keyword += ' ' + str(token)\n",
    "\n",
    "                elif (keyword != ' ') and (len(keyword.split()) >= min_words):\n",
    "                    keywords.append(keyword)\n",
    "                    keyword = ''\n",
    "        # Create dictionary of keywords and sort them to obtain the top n frequent keywords \n",
    "        keywords = dict(Counter(keywords))\n",
    "        keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:n_keywords]\n",
    "        \n",
    "        return keywords\n",
    "        \n",
    "with open('/srv/shares/NLP/wiki_nlp.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "    \n",
    "keywords = StopWordKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('words',) appears 273 times.\n",
    "The keyword ('text',) appears 263 times.\n",
    "The keyword ('example',) appears 257 times.\n",
    "The keyword ('word',) appears 201 times.\n",
    "The keyword ('references',) appears 184 times.\n",
    "The keyword ('natural', 'language', 'processing') appears 165 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "404059ce742f52d7a22c938834f96820",
     "grade": true,
     "grade_id": "test_Stopword_Keywords_A0",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b86c7fb9d2d8317a317e25292586a04",
     "grade": false,
     "grade_id": "Stopword_Keywords_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Stop word based keyword extraction B) [4 points]\n",
    "\n",
    "Rerun the keyword extrator with a minimum word count of ```min_words=2``` and a keyword count of ```n_keywords=15```.\n",
    "\n",
    "Store this in the variable ```keywords_2```. Print the result.\n",
    "\n",
    "Make sure to convert the input text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ace49c86890f478366272ba68cc1270",
     "grade": false,
     "grade_id": "Stopword_Keywords_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' natural language processing', 115),\n",
       " (' computational linguistics', 60),\n",
       " (' information retrieval', 49),\n",
       " (' references external links', 49),\n",
       " (' machine translation', 42),\n",
       " (' external links', 42),\n",
       " (' text mining', 37),\n",
       " (' word sense disambiguation', 36),\n",
       " (' machine learning', 34),\n",
       " (' natural language', 30),\n",
       " (' artificial intelligence', 29),\n",
       " (' sentiment analysis', 28),\n",
       " (' natural language understanding', 25),\n",
       " (' speech recognition', 23),\n",
       " (' information extraction', 22)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_2 = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "keywords_2 = StopWordKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=2)\n",
    "keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2557e9fa64432c99b589af4dc730fe3b",
     "grade": true,
     "grade_id": "test_Stopword_Keywords_B0",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
